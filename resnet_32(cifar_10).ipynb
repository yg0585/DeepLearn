{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RJPHSIttxbg",
        "outputId": "afe33b80-bd54-454a-ce6e-64bdb7f7a5d8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FfSglge_vtMX"
      },
      "outputs": [],
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range = 15,\n",
        "    horizontal_flip= True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "datagen.fit(x_train)\n",
        "\n",
        "x_train = x_train.astype(\"float32\")\n",
        "x_test = x_test.astype(\"float32\")\n",
        "mean = np.mean(x_train)\n",
        "std = np.std(x_train)\n",
        "x_test = (x_test - mean) / std\n",
        "x_train = (x_train - mean) / std\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v25iDGBu2Zow"
      },
      "outputs": [],
      "source": [
        "def resnet_block32(input_layer, filter, down_sample):\n",
        "  x = input_layer\n",
        "  for i in range(5):\n",
        "    identity_layer = x\n",
        "    x = tf.keras.layers.Conv2D(filters = filter, activation='relu', kernel_size = (3, 3), strides= (1, 1), padding='SAME', kernel_regularizer=tf.keras.regularizers.l2(0.0003), kernel_initializer=tf.keras.initializers.he_normal())(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters = filter, activation='relu', kernel_size = (3, 3), strides= (1, 1), padding='SAME', kernel_regularizer=tf.keras.regularizers.l2(0.0003), kernel_initializer=tf.keras.initializers.he_normal())(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    \n",
        "    identity_channel = identity_layer.shape.as_list()[-1]    \n",
        "    print(identity_channel)\n",
        "    if identity_channel != filter:\n",
        "      identity_layer = tf.keras.layers.Conv2D(filter, kernel_size=(1, 1), strides=(1, 1), padding= 'SAME')(identity_layer) \n",
        "    x = tf.keras.layers.Add()([x, identity_layer])\n",
        "  if down_sample == 0:\n",
        "    x = tf.keras.layers.Conv2D(filter, kernel_size = (1, 1), strides=(2, 2), padding='same')(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T7y0Lmd7w5XG"
      },
      "outputs": [],
      "source": [
        "x_input = tf.keras.layers.Input(shape = (32,32,3))\n",
        "x = tf.keras.layers.Conv2D(filters = 16, kernel_size = (3, 3), strides= (1, 1), padding = 'SAME',kernel_regularizer=tf.keras.regularizers.l2(0.0003), kernel_initializer=tf.keras.initializers.he_normal())(x_input)\n",
        "x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "x = tf.keras.layers.Activation('relu')(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A4zBCbxxn2E",
        "outputId": "07f712cc-079d-431a-c072-b0659564c2d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "filters = 16\n",
        "down_sample = [0, 0, 1]\n",
        "for i in range(3):\n",
        "  x = resnet_block32(x, filters, down_sample[i])\n",
        "  filters = filters * 2\n",
        "x = tf.keras.layers.AveragePooling2D(padding = 'SAME')(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(units = 10, activation = 'softmax', kernel_initializer=tf.keras.initializers.he_normal())(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=x_input, outputs = x)\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.SGD(lr=0.01, momentum=0.9),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "def lr_schedule(epoch):\n",
        "  if epoch > 120 :\n",
        "    return 0.0001\n",
        "  if epoch > 90 :\n",
        "    return 0.001\n",
        "  if epoch > 70 :\n",
        "    return 0.01\n",
        "  return 0.03\n",
        "  \n",
        "reduced_lr = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXw035kwDdcz",
        "outputId": "bb116729-8047-4bd7-de28-10aa7a40a270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 16)   0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 32, 32, 16)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 32, 32, 16)   0           ['dropout_2[0][0]',              \n",
            "                                                                  'activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 32, 32, 16)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 32, 32, 16)   0           ['dropout_4[0][0]',              \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 32, 32, 16)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 32, 32, 16)   0           ['dropout_6[0][0]',              \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 16)   2320        ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 32, 32, 16)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 32, 32, 16)   2320        ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 32, 32, 16)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 32, 32, 16)   0           ['dropout_8[0][0]',              \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 32, 32, 16)   2320        ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 32, 32, 16)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 16)   2320        ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 32, 32, 16)  64          ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 32, 32, 16)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 32, 32, 16)   0           ['dropout_10[0][0]',             \n",
            "                                                                  'add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 16, 16, 16)   272         ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 16, 16, 32)   4640        ['conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 16, 16, 32)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 16, 16, 32)   9248        ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 16, 16, 32)   0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 32)   544         ['conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 16, 16, 32)   0           ['dropout_12[0][0]',             \n",
            "                                                                  'conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 32)   9248        ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 32)  128         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 16, 16, 32)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 16, 16, 32)   9248        ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 32)  128         ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 16, 16, 32)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 16, 16, 32)   0           ['dropout_14[0][0]',             \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 32)   9248        ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16, 32)  128         ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 16, 16, 32)   0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 16, 16, 32)   9248        ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 16, 32)  128         ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 16, 16, 32)   0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 16, 16, 32)   0           ['dropout_16[0][0]',             \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 16, 16, 32)   9248        ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 16, 16, 32)  128         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 16, 16, 32)   0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 16, 16, 32)   9248        ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 16, 16, 32)  128         ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 16, 16, 32)   0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 16, 16, 32)   0           ['dropout_18[0][0]',             \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 16, 16, 32)   9248        ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 16, 16, 32)  128         ['conv2d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 16, 16, 32)   0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 16, 16, 32)   9248        ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 16, 16, 32)  128         ['conv2d_22[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 16, 16, 32)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 16, 16, 32)   0           ['dropout_20[0][0]',             \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 8, 8, 32)     1056        ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 8, 8, 64)     18496       ['conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 8, 8, 64)    256         ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 8, 8, 64)     0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 8, 8, 64)     36928       ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 8, 8, 64)    256         ['conv2d_25[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 8, 8, 64)     0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 8, 8, 64)     2112        ['conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 8, 8, 64)     0           ['dropout_22[0][0]',             \n",
            "                                                                  'conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 8, 8, 64)     36928       ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 8, 8, 64)    256         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 8, 8, 64)     0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 8, 8, 64)     36928       ['dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 8, 8, 64)    256         ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 8, 8, 64)     0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 8, 8, 64)     0           ['dropout_24[0][0]',             \n",
            "                                                                  'add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 8, 8, 64)     36928       ['add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 8, 8, 64)    256         ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 8, 8, 64)     0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 8, 8, 64)     36928       ['dropout_25[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 8, 8, 64)    256         ['conv2d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)           (None, 8, 8, 64)     0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 8, 8, 64)     0           ['dropout_26[0][0]',             \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 8, 8, 64)     36928       ['add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 8, 8, 64)    256         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)           (None, 8, 8, 64)     0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 8, 8, 64)     36928       ['dropout_27[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 8, 8, 64)    256         ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)           (None, 8, 8, 64)     0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 8, 8, 64)     0           ['dropout_28[0][0]',             \n",
            "                                                                  'add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 8, 8, 64)     36928       ['add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 8, 8, 64)    256         ['conv2d_33[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)           (None, 8, 8, 64)     0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 8, 8, 64)     36928       ['dropout_29[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 8, 8, 64)    256         ['conv2d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)           (None, 8, 8, 64)     0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 8, 8, 64)     0           ['dropout_30[0][0]',             \n",
            "                                                                  'add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 4, 4, 64)    0           ['add_14[0][0]']                 \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1024)         0           ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           10250       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 481,146\n",
            "Trainable params: 478,874\n",
            "Non-trainable params: 2,272\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6BZ_ErbEorE",
        "outputId": "63ffc26d-1c44-4792-875a-7cc1cdcb42df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "390/390 [==============================] - 28s 59ms/step - loss: 3.5224 - accuracy: 0.2730 - val_loss: 2.6580 - val_accuracy: 0.4014 - lr: 0.0300\n",
            "Epoch 2/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 2.5570 - accuracy: 0.4113 - val_loss: 2.2946 - val_accuracy: 0.4852 - lr: 0.0300\n",
            "Epoch 3/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 2.2807 - accuracy: 0.4706 - val_loss: 2.0270 - val_accuracy: 0.5367 - lr: 0.0300\n",
            "Epoch 4/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 2.0726 - accuracy: 0.5140 - val_loss: 1.7793 - val_accuracy: 0.6009 - lr: 0.0300\n",
            "Epoch 5/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.8899 - accuracy: 0.5499 - val_loss: 1.7402 - val_accuracy: 0.5986 - lr: 0.0300\n",
            "Epoch 6/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.7411 - accuracy: 0.5811 - val_loss: 1.5601 - val_accuracy: 0.6484 - lr: 0.0300\n",
            "Epoch 7/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.6127 - accuracy: 0.6064 - val_loss: 1.3674 - val_accuracy: 0.6946 - lr: 0.0300\n",
            "Epoch 8/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.5105 - accuracy: 0.6264 - val_loss: 1.3607 - val_accuracy: 0.6816 - lr: 0.0300\n",
            "Epoch 9/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.4195 - accuracy: 0.6456 - val_loss: 1.2610 - val_accuracy: 0.6989 - lr: 0.0300\n",
            "Epoch 10/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.3398 - accuracy: 0.6610 - val_loss: 1.1276 - val_accuracy: 0.7416 - lr: 0.0300\n",
            "Epoch 11/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.2712 - accuracy: 0.6781 - val_loss: 1.1127 - val_accuracy: 0.7324 - lr: 0.0300\n",
            "Epoch 12/200\n",
            "390/390 [==============================] - 21s 54ms/step - loss: 1.2288 - accuracy: 0.6837 - val_loss: 1.0700 - val_accuracy: 0.7437 - lr: 0.0300\n",
            "Epoch 13/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.1734 - accuracy: 0.6967 - val_loss: 0.9784 - val_accuracy: 0.7645 - lr: 0.0300\n",
            "Epoch 14/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.1291 - accuracy: 0.7054 - val_loss: 1.0020 - val_accuracy: 0.7562 - lr: 0.0300\n",
            "Epoch 15/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.0975 - accuracy: 0.7137 - val_loss: 0.9769 - val_accuracy: 0.7598 - lr: 0.0300\n",
            "Epoch 16/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.0626 - accuracy: 0.7219 - val_loss: 0.9687 - val_accuracy: 0.7658 - lr: 0.0300\n",
            "Epoch 17/200\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 1.0377 - accuracy: 0.7278 - val_loss: 0.9283 - val_accuracy: 0.7699 - lr: 0.0300\n",
            "Epoch 18/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.0229 - accuracy: 0.7292 - val_loss: 0.8827 - val_accuracy: 0.7862 - lr: 0.0300\n",
            "Epoch 19/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9988 - accuracy: 0.7368 - val_loss: 0.9711 - val_accuracy: 0.7609 - lr: 0.0300\n",
            "Epoch 20/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9795 - accuracy: 0.7415 - val_loss: 0.8575 - val_accuracy: 0.7903 - lr: 0.0300\n",
            "Epoch 21/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9662 - accuracy: 0.7451 - val_loss: 0.8629 - val_accuracy: 0.7906 - lr: 0.0300\n",
            "Epoch 22/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9512 - accuracy: 0.7529 - val_loss: 0.8381 - val_accuracy: 0.7988 - lr: 0.0300\n",
            "Epoch 23/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9375 - accuracy: 0.7553 - val_loss: 0.8797 - val_accuracy: 0.7819 - lr: 0.0300\n",
            "Epoch 24/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9305 - accuracy: 0.7565 - val_loss: 0.8457 - val_accuracy: 0.7923 - lr: 0.0300\n",
            "Epoch 25/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9193 - accuracy: 0.7591 - val_loss: 0.8539 - val_accuracy: 0.7939 - lr: 0.0300\n",
            "Epoch 26/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9084 - accuracy: 0.7645 - val_loss: 0.8775 - val_accuracy: 0.7867 - lr: 0.0300\n",
            "Epoch 27/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9007 - accuracy: 0.7681 - val_loss: 0.7823 - val_accuracy: 0.8109 - lr: 0.0300\n",
            "Epoch 28/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9036 - accuracy: 0.7653 - val_loss: 0.8723 - val_accuracy: 0.7825 - lr: 0.0300\n",
            "Epoch 29/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8962 - accuracy: 0.7692 - val_loss: 0.9464 - val_accuracy: 0.7640 - lr: 0.0300\n",
            "Epoch 30/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8948 - accuracy: 0.7698 - val_loss: 0.7996 - val_accuracy: 0.8117 - lr: 0.0300\n",
            "Epoch 31/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8864 - accuracy: 0.7724 - val_loss: 0.8109 - val_accuracy: 0.8027 - lr: 0.0300\n",
            "Epoch 32/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8863 - accuracy: 0.7728 - val_loss: 0.8097 - val_accuracy: 0.8070 - lr: 0.0300\n",
            "Epoch 33/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8773 - accuracy: 0.7754 - val_loss: 0.8484 - val_accuracy: 0.8003 - lr: 0.0300\n",
            "Epoch 34/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8732 - accuracy: 0.7788 - val_loss: 0.9549 - val_accuracy: 0.7754 - lr: 0.0300\n",
            "Epoch 35/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8736 - accuracy: 0.7765 - val_loss: 0.7609 - val_accuracy: 0.8286 - lr: 0.0300\n",
            "Epoch 36/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8637 - accuracy: 0.7812 - val_loss: 0.8839 - val_accuracy: 0.7867 - lr: 0.0300\n",
            "Epoch 37/200\n",
            "390/390 [==============================] - 21s 54ms/step - loss: 0.8696 - accuracy: 0.7792 - val_loss: 0.7873 - val_accuracy: 0.8196 - lr: 0.0300\n",
            "Epoch 38/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8706 - accuracy: 0.7804 - val_loss: 0.7982 - val_accuracy: 0.8124 - lr: 0.0300\n",
            "Epoch 39/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8654 - accuracy: 0.7834 - val_loss: 0.7789 - val_accuracy: 0.8227 - lr: 0.0300\n",
            "Epoch 40/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8672 - accuracy: 0.7825 - val_loss: 0.7902 - val_accuracy: 0.8150 - lr: 0.0300\n",
            "Epoch 41/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8573 - accuracy: 0.7864 - val_loss: 0.8627 - val_accuracy: 0.7992 - lr: 0.0300\n",
            "Epoch 42/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8613 - accuracy: 0.7839 - val_loss: 0.8410 - val_accuracy: 0.8005 - lr: 0.0300\n",
            "Epoch 43/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8521 - accuracy: 0.7889 - val_loss: 0.7652 - val_accuracy: 0.8241 - lr: 0.0300\n",
            "Epoch 44/200\n",
            "390/390 [==============================] - 21s 54ms/step - loss: 0.8529 - accuracy: 0.7881 - val_loss: 0.8537 - val_accuracy: 0.8059 - lr: 0.0300\n",
            "Epoch 45/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8574 - accuracy: 0.7849 - val_loss: 0.8208 - val_accuracy: 0.8094 - lr: 0.0300\n",
            "Epoch 46/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8507 - accuracy: 0.7878 - val_loss: 0.8540 - val_accuracy: 0.7961 - lr: 0.0300\n",
            "Epoch 47/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8482 - accuracy: 0.7899 - val_loss: 0.7355 - val_accuracy: 0.8348 - lr: 0.0300\n",
            "Epoch 48/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8474 - accuracy: 0.7906 - val_loss: 0.7798 - val_accuracy: 0.8193 - lr: 0.0300\n",
            "Epoch 49/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8398 - accuracy: 0.7927 - val_loss: 0.8136 - val_accuracy: 0.8125 - lr: 0.0300\n",
            "Epoch 50/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8433 - accuracy: 0.7943 - val_loss: 0.8326 - val_accuracy: 0.8073 - lr: 0.0300\n",
            "Epoch 51/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8468 - accuracy: 0.7921 - val_loss: 0.9294 - val_accuracy: 0.7797 - lr: 0.0300\n",
            "Epoch 52/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8425 - accuracy: 0.7944 - val_loss: 0.8217 - val_accuracy: 0.8105 - lr: 0.0300\n",
            "Epoch 53/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8496 - accuracy: 0.7912 - val_loss: 0.8190 - val_accuracy: 0.8109 - lr: 0.0300\n",
            "Epoch 54/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8326 - accuracy: 0.7951 - val_loss: 0.7951 - val_accuracy: 0.8249 - lr: 0.0300\n",
            "Epoch 55/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8383 - accuracy: 0.7967 - val_loss: 0.8863 - val_accuracy: 0.7970 - lr: 0.0300\n",
            "Epoch 56/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8375 - accuracy: 0.7959 - val_loss: 0.8010 - val_accuracy: 0.8173 - lr: 0.0300\n",
            "Epoch 57/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8372 - accuracy: 0.7965 - val_loss: 0.8155 - val_accuracy: 0.8188 - lr: 0.0300\n",
            "Epoch 58/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8394 - accuracy: 0.7958 - val_loss: 0.7275 - val_accuracy: 0.8440 - lr: 0.0300\n",
            "Epoch 59/200\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8356 - accuracy: 0.7975 - val_loss: 0.7373 - val_accuracy: 0.8378 - lr: 0.0300\n",
            "Epoch 60/200\n",
            "390/390 [==============================] - 23s 58ms/step - loss: 0.8334 - accuracy: 0.7983 - val_loss: 0.8294 - val_accuracy: 0.8171 - lr: 0.0300\n",
            "Epoch 61/200\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.8337 - accuracy: 0.7980 - val_loss: 0.8608 - val_accuracy: 0.8070 - lr: 0.0300\n",
            "Epoch 62/200\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.8370 - accuracy: 0.7962 - val_loss: 0.8063 - val_accuracy: 0.8205 - lr: 0.0300\n",
            "Epoch 63/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.8257 - accuracy: 0.8013 - val_loss: 0.8077 - val_accuracy: 0.8156 - lr: 0.0300\n",
            "Epoch 64/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.8361 - accuracy: 0.7979 - val_loss: 0.7869 - val_accuracy: 0.8191 - lr: 0.0300\n",
            "Epoch 65/200\n",
            "390/390 [==============================] - 21s 55ms/step - loss: 0.8285 - accuracy: 0.8004 - val_loss: 0.8311 - val_accuracy: 0.8100 - lr: 0.0300\n",
            "Epoch 66/200\n",
            "390/390 [==============================] - 22s 55ms/step - loss: 0.8302 - accuracy: 0.7996 - val_loss: 0.8346 - val_accuracy: 0.8091 - lr: 0.0300\n",
            "Epoch 67/200\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8253 - accuracy: 0.8007 - val_loss: 0.8007 - val_accuracy: 0.8184 - lr: 0.0300\n",
            "Epoch 68/200\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.8244 - accuracy: 0.8023 - val_loss: 0.7728 - val_accuracy: 0.8275 - lr: 0.0300\n",
            "Epoch 69/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.8266 - accuracy: 0.8008 - val_loss: 0.7293 - val_accuracy: 0.8466 - lr: 0.0300\n",
            "Epoch 70/200\n",
            "390/390 [==============================] - 21s 54ms/step - loss: 0.8263 - accuracy: 0.7991 - val_loss: 0.8317 - val_accuracy: 0.8215 - lr: 0.0300\n",
            "Epoch 71/200\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.8256 - accuracy: 0.8010 - val_loss: 0.7329 - val_accuracy: 0.8418 - lr: 0.0300\n",
            "Epoch 72/200\n",
            "390/390 [==============================] - 22s 55ms/step - loss: 0.7104 - accuracy: 0.8392 - val_loss: 0.6360 - val_accuracy: 0.8694 - lr: 0.0100\n",
            "Epoch 73/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.6621 - accuracy: 0.8551 - val_loss: 0.6146 - val_accuracy: 0.8740 - lr: 0.0100\n",
            "Epoch 74/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.6380 - accuracy: 0.8615 - val_loss: 0.5766 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "Epoch 75/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.6315 - accuracy: 0.8585 - val_loss: 0.6026 - val_accuracy: 0.8745 - lr: 0.0100\n",
            "Epoch 76/200\n",
            "390/390 [==============================] - 22s 55ms/step - loss: 0.6130 - accuracy: 0.8626 - val_loss: 0.5983 - val_accuracy: 0.8741 - lr: 0.0100\n",
            "Epoch 77/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.6053 - accuracy: 0.8638 - val_loss: 0.5649 - val_accuracy: 0.8825 - lr: 0.0100\n",
            "Epoch 78/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.5993 - accuracy: 0.8651 - val_loss: 0.5743 - val_accuracy: 0.8781 - lr: 0.0100\n",
            "Epoch 79/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.5933 - accuracy: 0.8648 - val_loss: 0.5642 - val_accuracy: 0.8794 - lr: 0.0100\n",
            "Epoch 80/200\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.5835 - accuracy: 0.8663 - val_loss: 0.5731 - val_accuracy: 0.8746 - lr: 0.0100\n",
            "Epoch 81/200\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.5808 - accuracy: 0.8649 - val_loss: 0.5763 - val_accuracy: 0.8764 - lr: 0.0100\n",
            "Epoch 82/200\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.5723 - accuracy: 0.8683 - val_loss: 0.5458 - val_accuracy: 0.8826 - lr: 0.0100\n",
            "Epoch 83/200\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5736 - accuracy: 0.8647 - val_loss: 0.5490 - val_accuracy: 0.8809 - lr: 0.0100\n",
            "Epoch 84/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.5711 - accuracy: 0.8654 - val_loss: 0.5583 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "Epoch 85/200\n",
            "390/390 [==============================] - 23s 58ms/step - loss: 0.5582 - accuracy: 0.8694 - val_loss: 0.5410 - val_accuracy: 0.8830 - lr: 0.0100\n",
            "Epoch 86/200\n",
            "390/390 [==============================] - 22s 58ms/step - loss: 0.5581 - accuracy: 0.8678 - val_loss: 0.5232 - val_accuracy: 0.8876 - lr: 0.0100\n",
            "Epoch 87/200\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5587 - accuracy: 0.8672 - val_loss: 0.5621 - val_accuracy: 0.8745 - lr: 0.0100\n",
            "Epoch 88/200\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.5555 - accuracy: 0.8682 - val_loss: 0.5349 - val_accuracy: 0.8792 - lr: 0.0100\n",
            "Epoch 89/200\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5553 - accuracy: 0.8672 - val_loss: 0.5586 - val_accuracy: 0.8754 - lr: 0.0100\n",
            "Epoch 90/200\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.5565 - accuracy: 0.8658 - val_loss: 0.5325 - val_accuracy: 0.8782 - lr: 0.0100\n",
            "Epoch 91/200\n",
            "390/390 [==============================] - 23s 58ms/step - loss: 0.5516 - accuracy: 0.8667 - val_loss: 0.5411 - val_accuracy: 0.8816 - lr: 0.0100\n",
            "Epoch 92/200\n",
            "390/390 [==============================] - 25s 63ms/step - loss: 0.5109 - accuracy: 0.8831 - val_loss: 0.4791 - val_accuracy: 0.8973 - lr: 0.0010\n",
            "Epoch 93/200\n",
            "390/390 [==============================] - 27s 68ms/step - loss: 0.4807 - accuracy: 0.8921 - val_loss: 0.4725 - val_accuracy: 0.8998 - lr: 0.0010\n",
            "Epoch 94/200\n",
            "390/390 [==============================] - 25s 65ms/step - loss: 0.4733 - accuracy: 0.8937 - val_loss: 0.4699 - val_accuracy: 0.8992 - lr: 0.0010\n",
            "Epoch 95/200\n",
            "390/390 [==============================] - 26s 66ms/step - loss: 0.4693 - accuracy: 0.8942 - val_loss: 0.4745 - val_accuracy: 0.8981 - lr: 0.0010\n",
            "Epoch 96/200\n",
            "390/390 [==============================] - 26s 68ms/step - loss: 0.4622 - accuracy: 0.8965 - val_loss: 0.4676 - val_accuracy: 0.8984 - lr: 0.0010\n",
            "Epoch 97/200\n",
            "390/390 [==============================] - 26s 66ms/step - loss: 0.4592 - accuracy: 0.8981 - val_loss: 0.4652 - val_accuracy: 0.8994 - lr: 0.0010\n",
            "Epoch 98/200\n",
            "390/390 [==============================] - 25s 63ms/step - loss: 0.4493 - accuracy: 0.9012 - val_loss: 0.4600 - val_accuracy: 0.9010 - lr: 0.0010\n",
            "Epoch 99/200\n",
            "390/390 [==============================] - 25s 65ms/step - loss: 0.4516 - accuracy: 0.9004 - val_loss: 0.4564 - val_accuracy: 0.9026 - lr: 0.0010\n",
            "Epoch 100/200\n",
            "390/390 [==============================] - 27s 68ms/step - loss: 0.4425 - accuracy: 0.9018 - val_loss: 0.4550 - val_accuracy: 0.9028 - lr: 0.0010\n",
            "Epoch 101/200\n",
            "390/390 [==============================] - 28s 71ms/step - loss: 0.4428 - accuracy: 0.9033 - val_loss: 0.4518 - val_accuracy: 0.9050 - lr: 0.0010\n",
            "Epoch 102/200\n",
            "390/390 [==============================] - 28s 72ms/step - loss: 0.4419 - accuracy: 0.9034 - val_loss: 0.4592 - val_accuracy: 0.9033 - lr: 0.0010\n",
            "Epoch 103/200\n",
            "390/390 [==============================] - 29s 73ms/step - loss: 0.4373 - accuracy: 0.9043 - val_loss: 0.4486 - val_accuracy: 0.9044 - lr: 0.0010\n",
            "Epoch 104/200\n",
            "390/390 [==============================] - 27s 68ms/step - loss: 0.4351 - accuracy: 0.9050 - val_loss: 0.4521 - val_accuracy: 0.9044 - lr: 0.0010\n",
            "Epoch 105/200\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.4278 - accuracy: 0.9075 - val_loss: 0.4448 - val_accuracy: 0.9057 - lr: 0.0010\n",
            "Epoch 106/200\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.4294 - accuracy: 0.9059 - val_loss: 0.4476 - val_accuracy: 0.9059 - lr: 0.0010\n",
            "Epoch 107/200\n",
            "390/390 [==============================] - 29s 74ms/step - loss: 0.4272 - accuracy: 0.9065 - val_loss: 0.4455 - val_accuracy: 0.9062 - lr: 0.0010\n",
            "Epoch 108/200\n",
            "390/390 [==============================] - 26s 68ms/step - loss: 0.4255 - accuracy: 0.9055 - val_loss: 0.4469 - val_accuracy: 0.9061 - lr: 0.0010\n",
            "Epoch 109/200\n",
            "390/390 [==============================] - 28s 71ms/step - loss: 0.4234 - accuracy: 0.9072 - val_loss: 0.4455 - val_accuracy: 0.9040 - lr: 0.0010\n",
            "Epoch 110/200\n",
            "390/390 [==============================] - 28s 72ms/step - loss: 0.4211 - accuracy: 0.9089 - val_loss: 0.4450 - val_accuracy: 0.9065 - lr: 0.0010\n",
            "Epoch 111/200\n",
            "390/390 [==============================] - 30s 78ms/step - loss: 0.4221 - accuracy: 0.9088 - val_loss: 0.4413 - val_accuracy: 0.9064 - lr: 0.0010\n",
            "Epoch 112/200\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.4150 - accuracy: 0.9118 - val_loss: 0.4426 - val_accuracy: 0.9069 - lr: 0.0010\n",
            "Epoch 113/200\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.4135 - accuracy: 0.9103 - val_loss: 0.4482 - val_accuracy: 0.9047 - lr: 0.0010\n",
            "Epoch 114/200\n",
            "390/390 [==============================] - 28s 73ms/step - loss: 0.4132 - accuracy: 0.9092 - val_loss: 0.4443 - val_accuracy: 0.9070 - lr: 0.0010\n",
            "Epoch 115/200\n",
            "390/390 [==============================] - 26s 65ms/step - loss: 0.4105 - accuracy: 0.9111 - val_loss: 0.4471 - val_accuracy: 0.9026 - lr: 0.0010\n",
            "Epoch 116/200\n",
            "390/390 [==============================] - 25s 64ms/step - loss: 0.4088 - accuracy: 0.9113 - val_loss: 0.4376 - val_accuracy: 0.9063 - lr: 0.0010\n",
            "Epoch 117/200\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.4088 - accuracy: 0.9113 - val_loss: 0.4382 - val_accuracy: 0.9069 - lr: 0.0010\n",
            "Epoch 118/200\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.4086 - accuracy: 0.9116 - val_loss: 0.4409 - val_accuracy: 0.9074 - lr: 0.0010\n",
            "Epoch 119/200\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.4062 - accuracy: 0.9107 - val_loss: 0.4340 - val_accuracy: 0.9077 - lr: 0.0010\n",
            "Epoch 120/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.4044 - accuracy: 0.9119 - val_loss: 0.4323 - val_accuracy: 0.9078 - lr: 0.0010\n",
            "Epoch 121/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.4010 - accuracy: 0.9124 - val_loss: 0.4350 - val_accuracy: 0.9080 - lr: 0.0010\n",
            "Epoch 122/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.4030 - accuracy: 0.9109 - val_loss: 0.4368 - val_accuracy: 0.9069 - lr: 0.0010\n",
            "Epoch 123/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3967 - accuracy: 0.9130 - val_loss: 0.4315 - val_accuracy: 0.9080 - lr: 0.0010\n",
            "Epoch 124/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3991 - accuracy: 0.9120 - val_loss: 0.4276 - val_accuracy: 0.9109 - lr: 0.0010\n",
            "Epoch 125/200\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.3951 - accuracy: 0.9137 - val_loss: 0.4298 - val_accuracy: 0.9095 - lr: 0.0010\n",
            "Epoch 126/200\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3927 - accuracy: 0.9136 - val_loss: 0.4307 - val_accuracy: 0.9078 - lr: 0.0010\n",
            "Epoch 127/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3950 - accuracy: 0.9139 - val_loss: 0.4303 - val_accuracy: 0.9080 - lr: 0.0010\n",
            "Epoch 128/200\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.3900 - accuracy: 0.9139 - val_loss: 0.4283 - val_accuracy: 0.9093 - lr: 0.0010\n",
            "Epoch 129/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3916 - accuracy: 0.9141 - val_loss: 0.4265 - val_accuracy: 0.9100 - lr: 0.0010\n",
            "Epoch 130/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3886 - accuracy: 0.9150 - val_loss: 0.4315 - val_accuracy: 0.9085 - lr: 0.0010\n",
            "Epoch 131/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3824 - accuracy: 0.9153 - val_loss: 0.4352 - val_accuracy: 0.9073 - lr: 0.0010\n",
            "Epoch 132/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3911 - accuracy: 0.9122 - val_loss: 0.4235 - val_accuracy: 0.9124 - lr: 0.0010\n",
            "Epoch 133/200\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.3882 - accuracy: 0.9129 - val_loss: 0.4250 - val_accuracy: 0.9110 - lr: 0.0010\n",
            "Epoch 134/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3856 - accuracy: 0.9137 - val_loss: 0.4247 - val_accuracy: 0.9083 - lr: 0.0010\n",
            "Epoch 135/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3831 - accuracy: 0.9157 - val_loss: 0.4217 - val_accuracy: 0.9099 - lr: 0.0010\n",
            "Epoch 136/200\n",
            "390/390 [==============================] - 30s 78ms/step - loss: 0.3771 - accuracy: 0.9171 - val_loss: 0.4257 - val_accuracy: 0.9090 - lr: 0.0010\n",
            "Epoch 137/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3772 - accuracy: 0.9155 - val_loss: 0.4179 - val_accuracy: 0.9109 - lr: 0.0010\n",
            "Epoch 138/200\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.3810 - accuracy: 0.9156 - val_loss: 0.4194 - val_accuracy: 0.9106 - lr: 0.0010\n",
            "Epoch 139/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3757 - accuracy: 0.9159 - val_loss: 0.4174 - val_accuracy: 0.9115 - lr: 0.0010\n",
            "Epoch 140/200\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.3765 - accuracy: 0.9167 - val_loss: 0.4182 - val_accuracy: 0.9112 - lr: 0.0010\n",
            "Epoch 141/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3748 - accuracy: 0.9179 - val_loss: 0.4190 - val_accuracy: 0.9099 - lr: 0.0010\n",
            "Epoch 142/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3760 - accuracy: 0.9169 - val_loss: 0.4125 - val_accuracy: 0.9093 - lr: 0.0010\n",
            "Epoch 143/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3672 - accuracy: 0.9201 - val_loss: 0.4212 - val_accuracy: 0.9080 - lr: 0.0010\n",
            "Epoch 144/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3721 - accuracy: 0.9164 - val_loss: 0.4127 - val_accuracy: 0.9124 - lr: 0.0010\n",
            "Epoch 145/200\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3716 - accuracy: 0.9173 - val_loss: 0.4111 - val_accuracy: 0.9130 - lr: 0.0010\n",
            "Epoch 146/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3694 - accuracy: 0.9172 - val_loss: 0.4203 - val_accuracy: 0.9076 - lr: 0.0010\n",
            "Epoch 147/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3658 - accuracy: 0.9189 - val_loss: 0.4175 - val_accuracy: 0.9087 - lr: 0.0010\n",
            "Epoch 148/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3690 - accuracy: 0.9173 - val_loss: 0.4153 - val_accuracy: 0.9106 - lr: 0.0010\n",
            "Epoch 149/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3689 - accuracy: 0.9185 - val_loss: 0.4211 - val_accuracy: 0.9075 - lr: 0.0010\n",
            "Epoch 150/200\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.3628 - accuracy: 0.9179 - val_loss: 0.4104 - val_accuracy: 0.9125 - lr: 0.0010\n",
            "Epoch 151/200\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3624 - accuracy: 0.9186 - val_loss: 0.4133 - val_accuracy: 0.9100 - lr: 0.0010\n",
            "Epoch 152/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3638 - accuracy: 0.9192 - val_loss: 0.4158 - val_accuracy: 0.9122 - lr: 0.0010\n",
            "Epoch 153/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3639 - accuracy: 0.9196 - val_loss: 0.4191 - val_accuracy: 0.9111 - lr: 0.0010\n",
            "Epoch 154/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3607 - accuracy: 0.9183 - val_loss: 0.4122 - val_accuracy: 0.9111 - lr: 0.0010\n",
            "Epoch 155/200\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3575 - accuracy: 0.9208 - val_loss: 0.4125 - val_accuracy: 0.9116 - lr: 0.0010\n",
            "Epoch 156/200\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3587 - accuracy: 0.9191 - val_loss: 0.4130 - val_accuracy: 0.9101 - lr: 0.0010\n",
            "Epoch 157/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3555 - accuracy: 0.9195 - val_loss: 0.4099 - val_accuracy: 0.9122 - lr: 0.0010\n",
            "Epoch 158/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3551 - accuracy: 0.9213 - val_loss: 0.4076 - val_accuracy: 0.9118 - lr: 0.0010\n",
            "Epoch 159/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3552 - accuracy: 0.9203 - val_loss: 0.4085 - val_accuracy: 0.9111 - lr: 0.0010\n",
            "Epoch 160/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3526 - accuracy: 0.9205 - val_loss: 0.4143 - val_accuracy: 0.9110 - lr: 0.0010\n",
            "Epoch 161/200\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3572 - accuracy: 0.9199 - val_loss: 0.4157 - val_accuracy: 0.9102 - lr: 0.0010\n",
            "Epoch 162/200\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.3539 - accuracy: 0.9199 - val_loss: 0.4110 - val_accuracy: 0.9122 - lr: 0.0010\n",
            "Epoch 163/200\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.3498 - accuracy: 0.9223 - val_loss: 0.4076 - val_accuracy: 0.9111 - lr: 0.0010\n",
            "Epoch 164/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3482 - accuracy: 0.9226 - val_loss: 0.4117 - val_accuracy: 0.9117 - lr: 0.0010\n",
            "Epoch 165/200\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.3483 - accuracy: 0.9215 - val_loss: 0.4078 - val_accuracy: 0.9086 - lr: 0.0010\n",
            "Epoch 166/200\n",
            "390/390 [==============================] - 30s 78ms/step - loss: 0.3493 - accuracy: 0.9221 - val_loss: 0.4055 - val_accuracy: 0.9125 - lr: 0.0010\n",
            "Epoch 167/200\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3466 - accuracy: 0.9218 - val_loss: 0.4105 - val_accuracy: 0.9113 - lr: 0.0010\n",
            "Epoch 168/200\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3448 - accuracy: 0.9224 - val_loss: 0.4086 - val_accuracy: 0.9098 - lr: 0.0010\n",
            "Epoch 169/200\n",
            "390/390 [==============================] - 28s 72ms/step - loss: 0.3445 - accuracy: 0.9234 - val_loss: 0.4047 - val_accuracy: 0.9132 - lr: 0.0010\n",
            "Epoch 170/200\n",
            "390/390 [==============================] - 28s 72ms/step - loss: 0.3427 - accuracy: 0.9229 - val_loss: 0.4007 - val_accuracy: 0.9134 - lr: 0.0010\n",
            "Epoch 171/200\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.3472 - accuracy: 0.9217 - val_loss: 0.4068 - val_accuracy: 0.9121 - lr: 0.0010\n",
            "Epoch 172/200\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.3418 - accuracy: 0.9221 - val_loss: 0.4051 - val_accuracy: 0.9135 - lr: 0.0010\n",
            "Epoch 173/200\n",
            "390/390 [==============================] - 28s 73ms/step - loss: 0.3381 - accuracy: 0.9253 - val_loss: 0.3984 - val_accuracy: 0.9143 - lr: 0.0010\n",
            "Epoch 174/200\n",
            "390/390 [==============================] - 30s 78ms/step - loss: 0.3377 - accuracy: 0.9251 - val_loss: 0.4013 - val_accuracy: 0.9116 - lr: 0.0010\n",
            "Epoch 175/200\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.3400 - accuracy: 0.9225 - val_loss: 0.4077 - val_accuracy: 0.9111 - lr: 0.0010\n",
            "Epoch 176/200\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.3354 - accuracy: 0.9232 - val_loss: 0.4000 - val_accuracy: 0.9109 - lr: 0.0010\n",
            "Epoch 177/200\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3395 - accuracy: 0.9226 - val_loss: 0.4058 - val_accuracy: 0.9091 - lr: 0.0010\n",
            "Epoch 178/200\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.3423 - accuracy: 0.9216 - val_loss: 0.4100 - val_accuracy: 0.9094 - lr: 0.0010\n",
            "Epoch 179/200\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.3363 - accuracy: 0.9253 - val_loss: 0.3982 - val_accuracy: 0.9125 - lr: 0.0010\n",
            "Epoch 180/200\n",
            " 40/390 [==>...........................] - ETA: 26s - loss: 0.3393 - accuracy: 0.9264"
          ]
        }
      ],
      "source": [
        "model.fit(datagen.flow(x_train, y_train, batch_size=128), callbacks=[reduced_lr],\n",
        "                       steps_per_epoch = len(x_train) // 128, epochs = 200, validation_data=(x_test, y_test), verbose = 1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "resnet_32(cifar 10)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
